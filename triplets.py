# -*- coding: utf-8 -*-
"""OWLV2_Triplets.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PntMrCvKrgczS6iYnL72VykwA35niVdz
"""

from transformers import Owlv2Processor, Owlv2ForObjectDetection
from PIL import Image, ImageDraw
import requests
import torch
from nltk import ngrams
import matplotlib.pyplot as plt
import numpy as np
from transformers.utils.constants import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD

processor = Owlv2Processor.from_pretrained("google/owlv2-base-patch16-ensemble")
model = Owlv2ForObjectDetection.from_pretrained("google/owlv2-base-patch16-ensemble")

url = 'http://images.cocodataset.org/val2017/000000039769.jpg'
image = Image.open(requests.get(url, stream=True).raw)
caption = "Cats are sleeping with remotes on a sofa"
image

# Define function to filter n-grams
def filter_ngrams(ngrams_list):
    filtered_ngrams = []
    uninformative_words = ["image", "jpg", "background", "wallpaper"]
    start_words = ["of", "on", "in"]
    end_words = ["A", "a", "the", "to", "on"]
    for ngram_list in ngrams_list:
        ngram_list = list(ngram_list)
        for ngram_tuple in ngram_list:
            is_informative = all(word not in uninformative_words for word in ngram_tuple)
            if is_informative and ngram_tuple[0] not in start_words and ngram_tuple[-1] not in end_words:
                filtered_ngrams.append(" ".join(ngram_tuple))
    return filtered_ngrams

# Generate n-grams from the caption
unigrams_to_eightgrams = [ngrams(caption.split(), n) for n in range(1, 9)]
filtered_ngrams = filter_ngrams(unigrams_to_eightgrams)
print("Filtered n-grams:", filtered_ngrams)

# Use OWLv2 to get bounding boxes
inputs = processor(text=filtered_ngrams, images=image, return_tensors="pt")
outputs = model(**inputs)
target_sizes = torch.Tensor([image.size[::-1]])
results = processor.post_process_object_detection(outputs=outputs, target_sizes=target_sizes, threshold=0.1)

# Generate triplets
print("Before generating triplets:")
print("Filtered n-grams length:", len(filtered_ngrams))
print("Results length:", len(results))
print("Triplets generated:")

triplets = []
# Iterate over both results and filtered n-grams
for i, (result, ngram) in enumerate(zip(results, filtered_ngrams)):
    # Check if there are more bounding boxes than filtered n-grams
    if i < len(results):
        boxes = result["boxes"]
        for box in boxes:
            # Create a triplet for each bounding box
            triplet = {
                "image": image,
                "bounding_box": box,
                "caption": ngram
            }
            triplets.append(triplet)
            # Increment the n-gram index
            i += 1
    else:
        break

print("Triplets length:", len(triplets))

# Define function to preprocess the image
def get_preprocessed_image(pixel_values):
    pixel_values = pixel_values.squeeze().numpy()
    unnormalized_image = (pixel_values * np.array(OPENAI_CLIP_STD)[:, None, None]) + np.array(OPENAI_CLIP_MEAN)[:, None, None]
    unnormalized_image = (unnormalized_image * 255).astype(np.uint8)
    unnormalized_image = np.moveaxis(unnormalized_image, 0, -1)
    unnormalized_image = Image.fromarray(unnormalized_image)
    return unnormalized_image

# Preprocess the image and obtain bounding boxes
unnormalized_image = get_preprocessed_image(inputs.pixel_values)
target_sizes = torch.Tensor([unnormalized_image.size[::-1]])
results = processor.post_process_object_detection(outputs=outputs, target_sizes=target_sizes, threshold=0.1)

# Print the results and unnormalized image for verification
print("Results:", results)
print("Unnormalized image size:", unnormalized_image.size)

# Load the original image
#image_path = 'path_to_your_image.jpg'  # Replace 'path_to_your_image.jpg' with the actual path to your image
original_image = unnormalized_image.copy()

# Create a draw object
draw = ImageDraw.Draw(original_image)

# Iterate through the detected objects and draw bounding boxes
for result, ngram in zip(results, filtered_ngrams):
    boxes = result["boxes"]
    labels = result["labels"]
    for box, label in zip(boxes, labels):
        box = [round(coord, 2) for coord in box.tolist()]
        x1, y1, x2, y2 = box
        draw.rectangle(xy=((x1, y1), (x2, y2)), outline="red")
        draw.text(xy=(x1, y1), text=ngram, fill="red")

# for result in results:
#     boxes = result['boxes']
#     for box in boxes:
#         # Unpack the box coordinates
#         x1, y1, x2, y2 = box.tolist()
#         # Draw a rectangle
#         draw.rectangle(xy=[(x1, y1), (x2, y2)], outline="red", width=2)

# Show the image with bounding boxes
original_image.show()

# Show the visualized image using matplotlib
resized_image = original_image.resize((10000, 10000))  # Adjust the size as needed

# Show the resized image
#resized_image.show()
plt.imshow(resized_image)
plt.axis('off')  # Hide axis
plt.show()

